{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WHwz96ZYqLE6"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "SzIRp6HxqqcM",
        "outputId": "d298ed1d-360e-43fa-b8f5-ee7395c32a13"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-86fac743-faac-4588-9bbd-7eedf3bf3400\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-86fac743-faac-4588-9bbd-7eedf3bf3400\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd4QGaV7SZ3G"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEByaJpCqwFx",
        "outputId": "0be7fa66-7eb6-47fe-d3d7-468591432541"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud) (11.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.25.3\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim wordcloud matplotlib nltk PyMuPDF\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91wTx1NQtVt1",
        "outputId": "a4224343-9487-4cd1-b907-0d8bfd9f827e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import fitz  # PyMuPDF for PDF reading\n",
        "from gensim.models import Word2Vec, Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqdebnPRuTsP",
        "outputId": "57333cc1-cad0-483c-8ce5-bf6a308cd4a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PDFs found: ['Unacademy-Compiler Design.pdf', 'WisserBalint-KurtiNelsonPhytopath06.pdf', '01.-Engineering-Mathematics_.pdf', 'A-review-on-threat-of-gray-leaf-spot-disease-of-maize-in-Asia.pdf', '08.-Compiler-Design_-1.pdf', '11.-Computer-Networks_.pdf', 'unacademy- Computer Networks (1).pdf', 'Unacademy-TOC.pdf', '09.-Operating-System_.pdf', '271_AI Lect Notes.pdf', 'file.pdf', 'Unacademy-Discrete Maths.pdf', 'prev year 2.pdf', 'Balint-KurtiJohalMaizeDisResist08.pdf', 'Unacademy-Digital Logic.pdf', '10.-DBMS_.pdf', '03.-Digital-Logic_.pdf', 'fpls-13-1064948.pdf', 'Unacademy-DBMS.pdf', 'prev year 1.pdf', 'g3journal2905.pdf', '12.-General-Aptitude_.pdf', 'jof-07-00989-v2.pdf', 'Unacademy-Programming & Data Structures (1).pdf', 'Unacademy-Algorithms (1).pdf', 'GATE _CS_2025_Syllabus.pdf', '06.-Algorithm_.pdf', '07.-Theory-of-Computation_.pdf', 'mpmi.1998.11.7.643.pdf', 'Unacademy-COA (1).pdf', '02.-Discrete-Mathematics_.pdf', 'Unacademy-Operating Systems (1).pdf', '1-s2.0-S1674205217300436-main.pdf', 'redinbaugh2014Controlofvirusdiseases.pdf', '04.-Computer-organization-Architecture_.pdf', '05.-Programming-Data-Structure_.pdf', 'Unacademy-Engineering Maths (1).pdf']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "folder_path = \"/content/\"  # If PDFs are directly in /content/\n",
        "pdf_files = [f for f in os.listdir(folder_path) if f.endswith(\".pdf\")]\n",
        "\n",
        "print(\"PDFs found:\", pdf_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHtrKoCmwV_o",
        "outputId": "8d97c4f1-c78f-4093-aefb-1638ba6ec2fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted text from 37 PDFs.\n"
          ]
        }
      ],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import glob\n",
        "\n",
        "def extract_text_from_pdfs(folder_path):\n",
        "    pdf_files = glob.glob(f\"{folder_path}/*.pdf\")\n",
        "    pdf_texts = []\n",
        "    for pdf_file in pdf_files:\n",
        "        with fitz.open(pdf_file) as doc:\n",
        "            text = \"\"\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "            pdf_texts.append(text)\n",
        "    return pdf_files, pdf_texts\n",
        "\n",
        "pdf_files, pdf_texts = extract_text_from_pdfs(folder_path)\n",
        "print(f\"Extracted text from {len(pdf_files)} PDFs.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "di5g4SxPwd21",
        "outputId": "f48e7c46-6371-402b-a232-441fc9dd6757"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PDF 1: /content/Unacademy-Compiler Design.pdf\n",
            "Introduction to Compiler Design\n",
            "Chapter 1\n",
            "1\n",
            "Language processing system:\n",
            "Pre-processor\n",
            "High level\n",
            "language\n",
            "Compiler\n",
            "Pure high\n",
            "level language\n",
            "Assembler\n",
            "Assembly\n",
            "code\n",
            "Relocatable\n",
            "machine code\n",
            "Linker\n",
            "Loader\n",
            "Additional ﬁle\n",
            "Object\n",
            "code\n",
            "Fig. 1.1 Language Processing System\n",
            "\t\n",
            "y\n",
            "The pre-processor is the first program to get invoked.\n",
            "\t\n",
            "y\n",
            "The main function of a pre-processor is to substitute the macros with \n",
            "the exact value.\n",
            "\t\n",
            "y\n",
            "The compiler is the next code to run. This program accepts a \n",
            "preprocessed file\n",
            "\n",
            "PDF 2: /content/WisserBalint-KurtiNelsonPhytopath06.pdf\n",
            "120 PHYTOPATHOLOGY \n",
            "Mini-Review \n",
            "The Genetic Architecture of Disease Resistance in Maize: \n",
            "A Synthesis of Published Studies \n",
            "Randall J. Wisser, Peter J. Balint-Kurti, and Rebecca J. Nelson \n",
            "First author: Department of Plant Breeding and Genetics, Institute for Genomic Diversity, 160 Biotechnology Building, Cornell University, \n",
            "Ithaca, NY 14853; second author: U.S. Department of Agriculture-Agricultural Research Service, 3418 Gardner Hall, Department of Plant \n",
            "Pathology, North Carolina State Univ\n",
            "\n",
            "PDF 3: /content/01.-Engineering-Mathematics_.pdf\n",
            "Engineering\n",
            "Engineering\n",
            "Mathematics\n",
            "Mathematics\n",
            "Engineering\n",
            "Mathematics\n",
            "Published By: \n",
            " \n",
            "  Physics Wallah \n",
            "ISBN: \n",
            "978-93-94342-39-2 \n",
            "Mobile App: \n",
            "Physics Wallah (Available on Play Store) \n",
            " \n",
            " \n",
            "Website: \n",
            "www.pw.live \n",
            "Email: \n",
            "support@pw.live \n",
            " \n",
            "Rights \n",
            "All rights will be reserved by Publisher. No part of this book may be used or reproduced in any manner \n",
            "whatsoever without the written permission from author or publisher. \n",
            "In the interest of student's community: \n",
            "Circulation of soft copy of Book(s) \n"
          ]
        }
      ],
      "source": [
        "for i, text in enumerate(pdf_texts[:3]):  # Show first 3 PDFs only\n",
        "    print(f\"\\nPDF {i+1}: {pdf_files[i]}\")\n",
        "    print(text[:500])  # Show first 500 characters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAMghS6mwskv",
        "outputId": "d527a5cb-7508-4b99-caec-920029dee22b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Download necessary NLTK data (only once)\n",
        "nltk.download('punkt')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_ORWV4Sw-bx",
        "outputId": "03da4c39-466f-49ae-b90d-25a1f8e34ef6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!rm -rf /root/nltk_data  # Purani nltk files delete karne ke liye\n",
        "!mkdir /root/nltk_data  # Naya nltk_data folder banane ke liye\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')  # Sent_tokenize aur word_tokenize ke liye zaroori\n",
        "nltk.download('averaged_perceptron_tagger')  # POS tagging ke liye\n",
        "nltk.download('stopwords')  # Stopwords removal ke liye\n",
        "nltk.download('wordnet')  # Lemmatization ke liye\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzie1-qAxK8q",
        "outputId": "9c53c57b-24a1-494a-d66f-b79ed3aa68e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentences: ['Hello!', 'This is a test sentence.', \"Let's check tokenization.\"]\n",
            "Words: ['Hello', '!', 'This', 'is', 'a', 'test', 'sentence', '.', 'Let', \"'s\", 'check', 'tokenization', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "test_text = \"Hello! This is a test sentence. Let's check tokenization.\"\n",
        "print(\"Sentences:\", sent_tokenize(test_text))\n",
        "print(\"Words:\", word_tokenize(test_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZYyP6NXyYGI",
        "outputId": "24fadce0-6179-478e-ee84-bde6a898cadc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample Tokenized Sentences (First PDF):\n",
            "['Introduction to Compiler Design\\nChapter 1\\n1\\nLanguage processing system:\\nPre-processor\\nHigh level\\nlanguage\\nCompiler\\nPure high\\nlevel language\\nAssembler\\nAssembly\\ncode\\nRelocatable\\nmachine code\\nLinker\\nLoader\\nAdditional ﬁle\\nObject\\ncode\\nFig.', '1.1 Language Processing System\\n\\t\\ny\\nThe pre-processor is the first program to get invoked.', 'y\\nThe main function of a pre-processor is to substitute the macros with \\nthe exact value.', 'y\\nThe compiler is the next code to run.', 'This program accepts a \\npreprocessed file as input and outputs assembly language.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Tokenize each PDF into sentences\n",
        "tokenized_sentences_per_pdf = [sent_tokenize(text) for text in pdf_texts]\n",
        "\n",
        "# Sample output: First 5 sentences from the first PDF\n",
        "print(\"\\nSample Tokenized Sentences (First PDF):\")\n",
        "print(tokenized_sentences_per_pdf[0][:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M84y_GoKzQSu",
        "outputId": "d4ae58ec-4a9b-49e1-b54f-70a4d5051323"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import csv\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Ensure necessary NLTK resources are downloaded\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize each PDF into sentences\n",
        "tokenized_sentences_per_pdf = [sent_tokenize(text) for text in pdf_texts]\n",
        "\n",
        "# Tokenize words for each sentence in each PDF\n",
        "tokenized_words_per_sentence = [\n",
        "    [[word.lower() for word in word_tokenize(sentence)] for sentence in sentences]\n",
        "    for sentences in tokenized_sentences_per_pdf\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgWnodKNzZ8d"
      },
      "outputs": [],
      "source": [
        "with open(\"tokenize.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, pdf_sentences in enumerate(tokenized_sentences_per_pdf):\n",
        "        f.write(f\"PDF {i+1}:\\n\")\n",
        "        for j, sentence in enumerate(pdf_sentences):\n",
        "            f.write(f\"Sentence {j+1}: {sentence}\\n\")\n",
        "        f.write(\"\\n\")  # Separate PDFs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBB6YRqhz_8K"
      },
      "outputs": [],
      "source": [
        "with open(\"tokenize.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"PDF Number\", \"Sentence Number\", \"Tokenized Sentence\"])  # Header\n",
        "\n",
        "    for i, pdf_sentences in enumerate(tokenized_sentences_per_pdf):\n",
        "        for j, sentence in enumerate(pdf_sentences):\n",
        "            writer.writerow([i+1, j+1, sentence])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcOyFNCt0FUI",
        "outputId": "fc7cc7d2-3128-489e-ec05-457ce91ac2ce"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import csv\n",
        "import string\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Get stopwords & punctuation list\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "punctuations = set(string.punctuation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jXCvfdp1tNk"
      },
      "outputs": [],
      "source": [
        "cleaned_lemmatized_words_per_pdf = []\n",
        "\n",
        "for text in pdf_texts:  # Loop through each PDF\n",
        "    sentences = sent_tokenize(text)  # Tokenize into sentences\n",
        "    lemmatized_words = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence.lower())  # Tokenize into words\n",
        "        words = [word for word in words if word not in stop_words and word not in punctuations]  # Remove stopwords & punctuation\n",
        "        lemmatized_words.extend([lemmatizer.lemmatize(word) for word in words])  # Apply lemmatization\n",
        "\n",
        "    cleaned_lemmatized_words_per_pdf.append(lemmatized_words)  # Store results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjFQHS2F1v1p"
      },
      "outputs": [],
      "source": [
        "with open(\"lemmatized_clean.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, words in enumerate(cleaned_lemmatized_words_per_pdf):\n",
        "        f.write(f\"PDF {i+1}:\\n\")\n",
        "        f.write(\" \".join(words) + \"\\n\\n\")  # Join words with space & separate PDFs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbK9sbnt1-U1"
      },
      "outputs": [],
      "source": [
        "with open(\"lemmatized_clean.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"PDF Number\", \"Lemmatized Text\"])  # Header\n",
        "\n",
        "    for i, words in enumerate(cleaned_lemmatized_words_per_pdf):\n",
        "        writer.writerow([i+1, \" \".join(words)])  # Write PDF number & words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5_UjJJV3cH5",
        "outputId": "74e42f17-c99a-4868-e8f6-340ec4e0ecf7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import csv\n",
        "import string\n",
        "import gensim\n",
        "import numpy as np\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec, Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Stopwords & punctuation removal\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "punctuations = set(string.punctuation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb0pP8KQ5DH_"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import sys\n",
        "\n",
        "# Increase CSV field size limit\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "# Read lemmatized text from CSV file\n",
        "lemmatized_texts = []\n",
        "with open(\"lemmatized_clean.csv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)  # Skip header\n",
        "    for row in reader:\n",
        "        lemmatized_texts.append(row[1].split())  # Convert text back into word lists\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh40BE4h5JE6"
      },
      "outputs": [],
      "source": [
        "# Train Word2Vec model on the lemmatized data\n",
        "word2vec_model = Word2Vec(sentences=lemmatized_texts, vector_size=100, window=5, min_count=2, workers=4)\n",
        "word2vec_model.save(\"word2vec.model\")\n",
        "\n",
        "# Save word vectors to CSV\n",
        "with open(\"word_vectors.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Word\", \"Vector\"])  # Header\n",
        "\n",
        "    for word in word2vec_model.wv.index_to_key:\n",
        "        writer.writerow([word, list(word2vec_model.wv[word])])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "654UoQNK5mWd"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}